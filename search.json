[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mariamaladsani.github.io",
    "section": "",
    "text": "Master’s student in UT Dallas’ Applied Cognition and Neuroscience program.\nContact: mariam.aladsani[at]utdallas.com"
  },
  {
    "objectID": "index.html#website-httpsmariamaladsani.github.io",
    "href": "index.html#website-httpsmariamaladsani.github.io",
    "title": "mariamaladsani.github.io",
    "section": "[Website] https://mariamaladsani.github.io",
    "text": "[Website] https://mariamaladsani.github.io"
  },
  {
    "objectID": "index.html#assignment-8-spatial-data",
    "href": "index.html#assignment-8-spatial-data",
    "title": "mariamaladsani.github.io",
    "section": "Assignment 8: Spatial Data",
    "text": "Assignment 8: Spatial Data\nAssignment 8: Spatial Data\nThe purpose of this assignment was to learn spatial data to learn the median age in the USA using the 2019 US Census Data. We start doing so by downloading the tidycensus and tigris packages. After downloading said packages, we can start observing the variables in our census dataset that will ultimately dictate the median age in each state. These variables account for sex, age, geography, among other important demographic information Additionally, access to US census data necessitates an API Key from Census API Data Service. After running, these vectors we can narrow our search to seek the median age data.\nBefore actualizing the data we run a plain map of the US:\nSince we have the schematic of the map, we are now able to apply the demographic variables we already ran on a map, which captures all of our data. I first ran the code for the median age by state for 2019.\nTo compare data to the median age by state in 2009, I tweaked the year in the code and generated this map:\nWhile, the map look like somewhat similar there are a few variable differences seen between 2009 and 2019, particularly in the Midwest where the median age is increasing. Regions like the east and west coast continue to maintain very similar demographics.\nTo compare further we I also tweaked the code to run the median age for the 2020 census data. Unfortunately, due to low-response rates the census just estimated the data. This is how it appeared in the console:\nThe regular 1-year ACS for 2020 was not released and is not available in tidycensus. ℹ Due to low response rates, the Census Bureau instead released a set of experimental estimates for the 2020 1-year ACS."
  },
  {
    "objectID": "index.html#assignment-6-webscrapingtextmining",
    "href": "index.html#assignment-6-webscrapingtextmining",
    "title": "mariamaladsani.github.io",
    "section": "Assignment 6: Webscraping/textmining",
    "text": "Assignment 6: Webscraping/textmining\nFor this project we webscraped Martin Luther King’s “I Have A Dream Speech,” using a text mining script. The text mining script downloads the “htmlTreeParse” function which allows to break down the article and ultimately process different vectors like punctuation, paragraphs, and word-frequency; which I will discuss later.\nOnce we install the necessary packages like XML, wordcloud, RColorBrwer, NLP, tm, and quantenda, we process the speech as an html document. This makes it easier to sparse through. As seen in the image below, the speech is now processed as an html document on RStudio.\nFollowing this procedure we start vectorizing the words in the document and we then run the words.corpus too. Both the words.vec and words.corpus are seen in the data section of RStudio, and reveal the language, content, length, as well as other features in MLK’s speech.\nWe run other elements of words.corpus like removing punctuating and numbers to make it easier to analyze the text. Running TermDocumentMatrix, processes minuscule elements of the speech like integers and characters, this tool is helpful for the generation of wordcloud later. After the intial run of TermDocumentMatrix we can no analyze more broader parts of the text like word count,.\nWe can finally generate a wordcloud of MLK’s “I Have A Dream,” speech, that displays the most frequent words used.\nDr. Ho, then showed me how to play around with word frequencies. We were able to generate a wordcloud with words that were only mentioned once in the speech. As seen, the wordcloud includes a wider variety of words.\nFor the second part of the project we were expected to follow the same code for Winston Churchill’s “The Finest Hour.” After processing the speech as html document. We were able to vectorize different aspects of the speech. This was the TermDocumentMatrix for the Finest Hours:\nThis is the word cloud generated for words with a frequency of 10. Similar to the MLK portion, I played around with the frequencies and inputted 5 instead of 10, and saw a significant difference. Both wordclouds can be seen below."
  },
  {
    "objectID": "index.html#assignment-3",
    "href": "index.html#assignment-3",
    "title": "mariamaladsani.github.io",
    "section": "Assignment 3",
    "text": "Assignment 3\nIn this assignment we were asked to analyze Biden-Xi summit that took place in November 2021. During this summit, Biden and Xi discussed China’s domestic policies, trade relationships, and security issues regarding the Asian region (Politico). For this assignment, we used the R alongside a Quantenda package to scrape and analyze Twitter data on the day of the summit. The twitter data analyzed displayed activity ranging from interactions, trending hashtags, popular users that were tagged, and prominent rhetoric on the day of the summit.\nThis is the summary of data:\nThe most popular hashtags on that were” #china”,” #joebiden”,” #xinjinping”, and “#america”. Other popular hashtags were “#usa,” “#breakingnews,” and “pray4america.” Quantenda data also pulled other popular hashtags from twitters among the ones not notes above are: “#uyghurgenocide,” “#uyghur,” “#humanrights,” “tibetans,” “taiwan,” “#coronavirus,” and interestingly enough “#fentanyl.” This plot shows how the variety of hashtags were used, and their connection with one another.\n\n\n\n\n\nThe Quantenda data allowed us to look at the 20 top users that were featured on the data matrix. The five top users mentioned were @POTUS, @JoeBiden, @Politico @EnesKanter, and @jendeben. The accompanying plot displays the interaction between various twitter users.\n\n\n\n\n\n\n\n\n\n\nAnother data point was analyzing lexical dispersion of US Presidents from 1953 to 2021. This plot ranged from 1953 when president Eisenhower was in office to current president Joe Biden. There is a lexical dispersion plot aimed to account for the frequency of times that presidents have used the term ‘american,’ in their speeches. In the plots this accounted for by referring to the term ‘relative token index.’ There is a wide variation of frequency among the US presidents, but according to the plot the use of the term does seem to slight increase around 1993 when Clinton was in office.\n\n\n\n\n\nThe second part of the lexical dispersion plot was comparing the frequencies between the words “american,” and “people.” The word “people,” showed less variation among presidents and was used quite frequently among all of them.\n\n\n\n\n\nLastly, we were asked to discuss Wordfish it. Wordfish analyzes positions of documents based on word frequencies. They do so by using a one-dimension scaling model."
  },
  {
    "objectID": "index.html#assignment-2",
    "href": "index.html#assignment-2",
    "title": "mariamaladsani.github.io",
    "section": "Assignment 2",
    "text": "Assignment 2\nI examined that number of searches for ‘Trump’ following the announcement of his election on June 6th 2015 for the 2016 US Election, up until 2023. I also examined the searches for ‘Biden’ following the same dates as my previous search.\nIn October and November 2016 there was a spike in searches for ‘Trump,’ coinciding to the election date. There was another sharp peak in ‘Trump,’ searches in October and November 2020. The search item ‘Biden’ was at its peak between the years of 2015 and 2023 in November 2020 during the election. Following the November 2020 election there was still more searches for ‘Biden,’ compared years 2015-early 2019, but not as frequent as the years of 2019 and 2020. The term ‘Election’ has a varied number of searches over time. The highest peaks being November 2016 and November 2020. There was a sharp dip of searches after January 2021, with some peaks in the searches in early to mid 2023.\nAll of the searches were exclusive to Google trends in the United States."
  },
  {
    "objectID": "index.html#assignment-1",
    "href": "index.html#assignment-1",
    "title": "mariamaladsani.github.io",
    "section": "Assignment 1",
    "text": "Assignment 1\nAnalysis of survey:\nQ2a. The survey is structured in a way where questions concerning the content is presented before the demographic information is asked.\nQ2b. The questionnaire is composed of an array of different questions, that range from binary answers like yes or no to Likert scales questions. The survey also includes questions that have logic, this always questions to be followed up or skipped based on questions with binary responses.\nQ2c. The questions are ordered so that the survey questions are presented first and the demographics are placed last.\nQ7. One way to improve a respondent’s experience is to break up the survey into sections to maintain cohesion and synchronicity throughout the survey.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am currently a master’s student at the University of Texas at Dallas’ Applied Cognition and Neuroscience program. In the spring of 2022, I graduated from Boston University with a BA in Psychology and a double minor in Philosophy and Comparative Literature. During my undergraduate career, I partook in research involving molecular neuroscience and clinical neuropsychology. In Dr. Ryan Logan’s lab in the Boston University School of Medicine, I worked on a project that investigated the role of circadian rhythms in opiod-seeking, craving, and relapse. At the Manoach Lab at Massachusetts General Hospital, I worked on a study investigating the relationship between memory consolidation and sleep in schizophrenic patients using EEG, MRI, and MEG, as well as neuropsychological assessments.\nDuring my graduate studies I am hoping to bridge my interests in both molecular neuroscience and neuropsychology. I am interested in how social and environmental factors in early life can impact and/or impair cognitive functions like memory later on in life. Overall, I aspire to be a well-rounded neuroscientist by conducting research that draws from different disciplines and methods.\nYou can find my CV here.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "assign01.html",
    "href": "assign01.html",
    "title": "assign01",
    "section": "",
    "text": "Analysis of survey:\nQ2a. The survey is structured in a way where questions concerning the content is presented before the demographic information is asked.\nQ2b. The questionnaire is composed of an array of different questions, that range from binary answers like yes or no to Likert scales questions. The survey also includes questions that have logic, this always questions to be followed up or skipped based on questions with binary responses.\nQ2c. The questions are ordered so that the survey questions are presented first and the demographics are placed last.\nQ7. One way to improve a respondent’s experience is to break up the survey into sections to maintain cohesion and synchronicity throughout the survey."
  },
  {
    "objectID": "assign01.html#assignment-1-surveys",
    "href": "assign01.html#assignment-1-surveys",
    "title": "assign01",
    "section": "",
    "text": "Analysis of survey:\nQ2a. The survey is structured in a way where questions concerning the content is presented before the demographic information is asked.\nQ2b. The questionnaire is composed of an array of different questions, that range from binary answers like yes or no to Likert scales questions. The survey also includes questions that have logic, this always questions to be followed up or skipped based on questions with binary responses.\nQ2c. The questions are ordered so that the survey questions are presented first and the demographics are placed last.\nQ7. One way to improve a respondent’s experience is to break up the survey into sections to maintain cohesion and synchronicity throughout the survey."
  },
  {
    "objectID": "assign02.html",
    "href": "assign02.html",
    "title": "assign02",
    "section": "",
    "text": "I examined that number of searches for ‘Trump’ following the announcement of his election on June 6th 2015 for the 2016 US Election, up until 2023. I also examined the searches for ‘Biden’ following the same dates as my previous search.\nIn October and November 2016 there was a spike in searches for ‘Trump,’ coinciding to the election date. There was another sharp peak in ‘Trump,’ searches in October and November 2020. The search item ‘Biden’ was at its peak between the years of 2015 and 2023 in November 2020 during the election. Following the November 2020 election there was still more searches for ‘Biden,’ compared years 2015-early 2019, but not as frequent as the years of 2019 and 2020. The term ‘Election’ has a varied number of searches over time. The highest peaks being November 2016 and November 2020. There was a sharp dip of searches after January 2021, with some peaks in the searches in early to mid 2023.\nAll of the searches were exclusive to Google trends in the United States."
  },
  {
    "objectID": "assign02.html#assignment-2-gtrends",
    "href": "assign02.html#assignment-2-gtrends",
    "title": "assign02",
    "section": "",
    "text": "I examined that number of searches for ‘Trump’ following the announcement of his election on June 6th 2015 for the 2016 US Election, up until 2023. I also examined the searches for ‘Biden’ following the same dates as my previous search.\nIn October and November 2016 there was a spike in searches for ‘Trump,’ coinciding to the election date. There was another sharp peak in ‘Trump,’ searches in October and November 2020. The search item ‘Biden’ was at its peak between the years of 2015 and 2023 in November 2020 during the election. Following the November 2020 election there was still more searches for ‘Biden,’ compared years 2015-early 2019, but not as frequent as the years of 2019 and 2020. The term ‘Election’ has a varied number of searches over time. The highest peaks being November 2016 and November 2020. There was a sharp dip of searches after January 2021, with some peaks in the searches in early to mid 2023.\nAll of the searches were exclusive to Google trends in the United States."
  },
  {
    "objectID": "assign03.html",
    "href": "assign03.html",
    "title": "assign03",
    "section": "",
    "text": "In this assignment we were asked to analyze Biden-Xi summit that took place in November 2021. During this summit, Biden and Xi discussed China’s domestic policies, trade relationships, and security issues regarding the Asian region (Politico). For this assignment, we used the R alongside a Quantenda package to scrape and analyze Twitter data on the day of the summit. The twitter data analyzed displayed activity ranging from interactions, trending hashtags, popular users that were tagged, and p\nrominent rhetoric on the day of the summit.\nThis is the summary of data:\nThe most popular hashtags on that were” #china”,” #joebiden”,” #xinjinping”, and “#america”. Other popular hashtags were “#usa,” “#breakingnews,” and “pray4america.” Quantenda data also pulled other popular hashtags from twitters among the ones not notes above are: “#uyghurgenocide,” “#uyghur,” “#humanrights,” “tibetans,” “taiwan,” “#coronavirus,” and interestingly enough “#fentanyl.” This plot shows how the variety of hashtags were used, and their connection with one another.\n\n\n\n\n\nThe Quantenda data allowed us to look at the 20 top users that were featured on the data matrix. The five top users mentioned were @POTUS, @JoeBiden, @Politico @EnesKanter, and @jendeben. The accompanying plot displays the interaction between various twitter users.\n\n\n\n\n\n\n\n\n\n\nAnother data point was analyzing lexical dispersion of US Presidents from 1953 to 2021. This plot ranged from 1953 when president Eisenhower was in office to current president Joe Biden. There is a lexical dispersion plot aimed to account for the frequency of times that presidents have used the term ‘american,’ in their speeches. In the plots this accounted for by referring to the term ‘relative token index.’ There is a wide variation of frequency among the US presidents, but according to the plot the use of the term does seem to slight increase around 1993 when Clinton was in office.\n\n\n\n\n\nThe second part of the lexical dispersion plot was comparing the frequencies between the words “american,” and “people.” The word “people,” showed less variation among presidents and was used quite frequently among all of them.\n\n\n\n\n\nLastly, we were asked to discuss Wordfish it. Wordfish analyzes positions of documents based on word frequencies. They do so by using a one-dimension scaling model."
  },
  {
    "objectID": "assign03.html#assignment-3-quantenda",
    "href": "assign03.html#assignment-3-quantenda",
    "title": "assign03",
    "section": "",
    "text": "In this assignment we were asked to analyze Biden-Xi summit that took place in November 2021. During this summit, Biden and Xi discussed China’s domestic policies, trade relationships, and security issues regarding the Asian region (Politico). For this assignment, we used the R alongside a Quantenda package to scrape and analyze Twitter data on the day of the summit. The twitter data analyzed displayed activity ranging from interactions, trending hashtags, popular users that were tagged, and p\nrominent rhetoric on the day of the summit.\nThis is the summary of data:\nThe most popular hashtags on that were” #china”,” #joebiden”,” #xinjinping”, and “#america”. Other popular hashtags were “#usa,” “#breakingnews,” and “pray4america.” Quantenda data also pulled other popular hashtags from twitters among the ones not notes above are: “#uyghurgenocide,” “#uyghur,” “#humanrights,” “tibetans,” “taiwan,” “#coronavirus,” and interestingly enough “#fentanyl.” This plot shows how the variety of hashtags were used, and their connection with one another.\n\n\n\n\n\nThe Quantenda data allowed us to look at the 20 top users that were featured on the data matrix. The five top users mentioned were @POTUS, @JoeBiden, @Politico @EnesKanter, and @jendeben. The accompanying plot displays the interaction between various twitter users.\n\n\n\n\n\n\n\n\n\n\nAnother data point was analyzing lexical dispersion of US Presidents from 1953 to 2021. This plot ranged from 1953 when president Eisenhower was in office to current president Joe Biden. There is a lexical dispersion plot aimed to account for the frequency of times that presidents have used the term ‘american,’ in their speeches. In the plots this accounted for by referring to the term ‘relative token index.’ There is a wide variation of frequency among the US presidents, but according to the plot the use of the term does seem to slight increase around 1993 when Clinton was in office.\n\n\n\n\n\nThe second part of the lexical dispersion plot was comparing the frequencies between the words “american,” and “people.” The word “people,” showed less variation among presidents and was used quite frequently among all of them.\n\n\n\n\n\nLastly, we were asked to discuss Wordfish it. Wordfish analyzes positions of documents based on word frequencies. They do so by using a one-dimension scaling model."
  },
  {
    "objectID": "assign06.html",
    "href": "assign06.html",
    "title": "assign06",
    "section": "",
    "text": "For this project we webscraped Martin Luther King’s “I Have A Dream Speech,” using a text mining script. The text mining script downloads the “htmlTreeParse” function which allows to break down the article and ultimately process different vectors like punctuation, paragraphs, and word-frequency; which I will discuss later.\nOnce we install the necessary packages like XML, wordcloud, RColorBrwer, NLP, tm, and quantenda, we process the speech as an html document. This makes it easier to sparse through. As seen in the image below, the speech is now processed as an html document on RStudio.\n\n\n\n\n\nFollowing this procedure we start vectorizing the words in the document and we then run the words.corpus too. Both the words.vec and words.corpus are seen in the data section of RStudio, and reveal the language, content, length, as well as other features in MLK’s speech.\nWe run other elements of words.corpus like removing punctuating and numbers to make it easier to analyze the text. Running TermDocumentMatrix, processes minuscule elements of the speech like integers and characters, this tool is helpful for the generation of wordcloud later. After the intial run of TermDocumentMatrix we can no analyze more broader parts of the text like word count,.\n\n\n\n\n\nWe can finally generate a wordcloud of MLK’s “I Have A Dream,” speech, that displays the most frequent words used.\n\n\n\n\n\nDr. Ho, then showed me how to play around with word frequencies. We were able to generate a wordcloud with words that were only mentioned once in the speech. As seen, the wordcloud includes a wider variety of words.\n\n\n\n\n\nFor the second part of the project we were expected to follow the same code for Winston Churchill’s “The Finest Hour.” After processing the speech as html document. We were able to vectorize different aspects of the speech. This was the TermDocumentMatrix for the Finest Hours:\n\n\n\n\n\nThis is the word cloud generated for words with a frequency of 10. Similar to the MLK portion, I played around with the frequencies and inputted 5 instead of 10, and saw a significant difference. Both wordclouds can be seen below."
  },
  {
    "objectID": "assign06.html#assignment-6-webscrapingtextmining",
    "href": "assign06.html#assignment-6-webscrapingtextmining",
    "title": "assign06",
    "section": "",
    "text": "For this project we webscraped Martin Luther King’s “I Have A Dream Speech,” using a text mining script. The text mining script downloads the “htmlTreeParse” function which allows to break down the article and ultimately process different vectors like punctuation, paragraphs, and word-frequency; which I will discuss later.\nOnce we install the necessary packages like XML, wordcloud, RColorBrwer, NLP, tm, and quantenda, we process the speech as an html document. This makes it easier to sparse through. As seen in the image below, the speech is now processed as an html document on RStudio.\n\n\n\n\n\nFollowing this procedure we start vectorizing the words in the document and we then run the words.corpus too. Both the words.vec and words.corpus are seen in the data section of RStudio, and reveal the language, content, length, as well as other features in MLK’s speech.\nWe run other elements of words.corpus like removing punctuating and numbers to make it easier to analyze the text. Running TermDocumentMatrix, processes minuscule elements of the speech like integers and characters, this tool is helpful for the generation of wordcloud later. After the intial run of TermDocumentMatrix we can no analyze more broader parts of the text like word count,.\n\n\n\n\n\nWe can finally generate a wordcloud of MLK’s “I Have A Dream,” speech, that displays the most frequent words used.\n\n\n\n\n\nDr. Ho, then showed me how to play around with word frequencies. We were able to generate a wordcloud with words that were only mentioned once in the speech. As seen, the wordcloud includes a wider variety of words.\n\n\n\n\n\nFor the second part of the project we were expected to follow the same code for Winston Churchill’s “The Finest Hour.” After processing the speech as html document. We were able to vectorize different aspects of the speech. This was the TermDocumentMatrix for the Finest Hours:\n\n\n\n\n\nThis is the word cloud generated for words with a frequency of 10. Similar to the MLK portion, I played around with the frequencies and inputted 5 instead of 10, and saw a significant difference. Both wordclouds can be seen below."
  },
  {
    "objectID": "assign08.html",
    "href": "assign08.html",
    "title": "assign08",
    "section": "",
    "text": "The purpose of this assignment was to learn spatial data to learn the median age in the USA using the 2019 US Census Data. We start doing so by downloading the tidycensus and tigris packages. After downloading said packages, we can start observing the variables in our census dataset that will ultimately dictate the median age in each state. These variables account for sex, age, geography, among other important demographic information Additionally, access to US census data necessitates an API Key from Census API Data Service. After running, these vectors we can narrow our search to seek the median age data.\nBefore actualizing the data we run a plain map of the US:\n\nSince we have the schematic of the map, we are now able to apply the demographic variables we already ran on a map, which captures all of our data. I first ran the code for the median age by state for 2019.\n\n\n\n\n\nTo compare data to the median age by state in 2009, I tweaked the year in the code and generated this map:\n\n\n\n\n\nWhile, the map look like somewhat similar there are a few variable differences seen between 2009 and 2019, particularly in the Midwest where the median age is increasing. Regions like the east and west coast continue to maintain very similar demographics.\nTo compare further we I also tweaked the code to run the median age for the 2020 census data. Unfortunately, due to low-response rates the census just estimated the data. This is how it appeared in the console:\nThe regular 1-year ACS for 2020 was not released and is not available in tidycensus. ℹ Due to low response rates, the Census Bureau instead released a set of experimental estimates for the 2020 1-year ACS."
  },
  {
    "objectID": "assign08.html#assignment-8-spatial-data",
    "href": "assign08.html#assignment-8-spatial-data",
    "title": "assign08",
    "section": "",
    "text": "The purpose of this assignment was to learn spatial data to learn the median age in the USA using the 2019 US Census Data. We start doing so by downloading the tidycensus and tigris packages. After downloading said packages, we can start observing the variables in our census dataset that will ultimately dictate the median age in each state. These variables account for sex, age, geography, among other important demographic information Additionally, access to US census data necessitates an API Key from Census API Data Service. After running, these vectors we can narrow our search to seek the median age data.\nBefore actualizing the data we run a plain map of the US:\n\nSince we have the schematic of the map, we are now able to apply the demographic variables we already ran on a map, which captures all of our data. I first ran the code for the median age by state for 2019.\n\n\n\n\n\nTo compare data to the median age by state in 2009, I tweaked the year in the code and generated this map:\n\n\n\n\n\nWhile, the map look like somewhat similar there are a few variable differences seen between 2009 and 2019, particularly in the Midwest where the median age is increasing. Regions like the east and west coast continue to maintain very similar demographics.\nTo compare further we I also tweaked the code to run the median age for the 2020 census data. Unfortunately, due to low-response rates the census just estimated the data. This is how it appeared in the console:\nThe regular 1-year ACS for 2020 was not released and is not available in tidycensus. ℹ Due to low response rates, the Census Bureau instead released a set of experimental estimates for the 2020 1-year ACS."
  },
  {
    "objectID": "finalproject.html",
    "href": "finalproject.html",
    "title": "EPPS 6302 Final Project",
    "section": "",
    "text": "Our Methods of Data Collection and Production sought to study the lifestyle impacts of heat-induced climate change on DFW-residents. We conducted our research by utilizing medical and climate data, as well as distributing a survey to UTD students You can read our research paper and view our slides below.\nEPPS 6302: Final Project Presentation\n&lt;iframe src=“” width=“600” height=“400”&gt;&lt;/iframe&gt;"
  },
  {
    "objectID": "assign07.html",
    "href": "assign07.html",
    "title": "assign07",
    "section": "",
    "text": "This assignment required us to download large data sets from government websites. Prior to downloading any of the data we reset the console to conserve memory. After resetting, we downloaded the purrr and magrittr packages.\n\n\n\n\n\nI set the path to my desktop directory. After completing that, I downloaded congressional hearing datasets from 2021-2023 from https://www.govinfo.gov/app/search/ . After downloading, I was able to download and the organize the data on RStudio.\n\n\n\n\n\nRStudio was also able to organize other variables of the dataset in the Data tab.\n\n\n\n\n\nThe values set allows to see how long the dataset took long to load, in this case it took 9 seconds. We were also suggested to look at other storage methods like: Arrow, Feather, and Parquet. Arrow is a storage method that is able to hierarchically organize data on modern hardware. Feather is able “to read and write feather files,” at “maximum speed.” Lastly, Parquet similar to feather also manages columnar storage datasets, other than it also runs on Python."
  },
  {
    "objectID": "assign07.html#assignment-7-government-data-and-data-processing",
    "href": "assign07.html#assignment-7-government-data-and-data-processing",
    "title": "assign07",
    "section": "",
    "text": "This assignment required us to download large data sets from government websites. Prior to downloading any of the data we reset the console to conserve memory. After resetting, we downloaded the purrr and magrittr packages.\n\n\n\n\n\nI set the path to my desktop directory. After completing that, I downloaded congressional hearing datasets from 2021-2023 from https://www.govinfo.gov/app/search/ . After downloading, I was able to download and the organize the data on RStudio.\n\n\n\n\n\nRStudio was also able to organize other variables of the dataset in the Data tab.\n\n\n\n\n\nThe values set allows to see how long the dataset took long to load, in this case it took 9 seconds. We were also suggested to look at other storage methods like: Arrow, Feather, and Parquet. Arrow is a storage method that is able to hierarchically organize data on modern hardware. Feather is able “to read and write feather files,” at “maximum speed.” Lastly, Parquet similar to feather also manages columnar storage datasets, other than it also runs on Python."
  },
  {
    "objectID": "index.html#assignment-3-quanteda",
    "href": "index.html#assignment-3-quanteda",
    "title": "mariamaladsani.github.io",
    "section": "Assignment 3: Quanteda",
    "text": "Assignment 3: Quanteda\nIn this assignment we were asked to analyze Biden-Xi summit that took place in November 2021. During this summit, Biden and Xi discussed China’s domestic policies, trade relationships, and security issues regarding the Asian region (Politico). For this assignment, we used the R alongside a Quantenda package to scrape and analyze Twitter data on the day of the summit. The twitter data analyzed displayed activity ranging from interactions, trending hashtags, popular users that were tagged, and prominent rhetoric on the day of the summit.\nThis is the summary of data:\nThe most popular hashtags on that were” #china”,” #joebiden”,” #xinjinping”, and “#america”. Other popular hashtags were “#usa,” “#breakingnews,” and “pray4america.” Quantenda data also pulled other popular hashtags from twitters among the ones not notes above are: “#uyghurgenocide,” “#uyghur,” “#humanrights,” “tibetans,” “taiwan,” “#coronavirus,” and interestingly enough “#fentanyl.” This plot shows how the variety of hashtags were used, and their connection with one another.\n\n\n\n\n\nThe Quantenda data allowed us to look at the 20 top users that were featured on the data matrix. The five top users mentioned were @POTUS, @JoeBiden, @Politico @EnesKanter, and @jendeben. The accompanying plot displays the interaction between various twitter users.\n\n\n\n\n\n\n\n\n\n\nAnother data point was analyzing lexical dispersion of US Presidents from 1953 to 2021. This plot ranged from 1953 when president Eisenhower was in office to current president Joe Biden. There is a lexical dispersion plot aimed to account for the frequency of times that presidents have used the term ‘american,’ in their speeches. In the plots this accounted for by referring to the term ‘relative token index.’ There is a wide variation of frequency among the US presidents, but according to the plot the use of the term does seem to slight increase around 1993 when Clinton was in office.\n\n\n\n\n\nThe second part of the lexical dispersion plot was comparing the frequencies between the words “american,” and “people.” The word “people,” showed less variation among presidents and was used quite frequently among all of them.\n\n\n\n\n\nLastly, we were asked to discuss Wordfish it. Wordfish analyzes positions of documents based on word frequencies. They do so by using a one-dimension scaling model."
  },
  {
    "objectID": "index.html#assignment-2-g-trends",
    "href": "index.html#assignment-2-g-trends",
    "title": "mariamaladsani.github.io",
    "section": "Assignment 2: G-Trends",
    "text": "Assignment 2: G-Trends\nI examined that number of searches for ‘Trump’ following the announcement of his election on June 6th 2015 for the 2016 US Election, up until 2023. I also examined the searches for ‘Biden’ following the same dates as my previous search.\nIn October and November 2016 there was a spike in searches for ‘Trump,’ coinciding to the election date. There was another sharp peak in ‘Trump,’ searches in October and November 2020. The search item ‘Biden’ was at its peak between the years of 2015 and 2023 in November 2020 during the election. Following the November 2020 election there was still more searches for ‘Biden,’ compared years 2015-early 2019, but not as frequent as the years of 2019 and 2020. The term ‘Election’ has a varied number of searches over time. The highest peaks being November 2016 and November 2020. There was a sharp dip of searches after January 2021, with some peaks in the searches in early to mid 2023.\nAll of the searches were exclusive to Google trends in the United States."
  },
  {
    "objectID": "index.html#assignment-1-survey-analysis",
    "href": "index.html#assignment-1-survey-analysis",
    "title": "mariamaladsani.github.io",
    "section": "Assignment 1: Survey Analysis",
    "text": "Assignment 1: Survey Analysis\nAnalysis of survey:\nQ2a. The survey is structured in a way where questions concerning the content is presented before the demographic information is asked.\nQ2b. The questionnaire is composed of an array of different questions, that range from binary answers like yes or no to Likert scales questions. The survey also includes questions that have logic, this always questions to be followed up or skipped based on questions with binary responses.\nQ2c. The questions are ordered so that the survey questions are presented first and the demographics are placed last.\nQ7. One way to improve a respondent’s experience is to break up the survey into sections to maintain cohesion and synchronicity throughout the survey.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#assignment-7-government-data-and-data-processing",
    "href": "index.html#assignment-7-government-data-and-data-processing",
    "title": "mariamaladsani.github.io",
    "section": "Assignment 7: Government data and data processing",
    "text": "Assignment 7: Government data and data processing\nThis assignment required us to download large data sets from government websites. Prior to downloading any of the data we reset the console to conserve memory. After resetting, we downloaded the purrr and magrittr packages.\nI set the path to my desktop directory. After completing that, I downloaded congressional hearing datasets from 2021-2023 from https://www.govinfo.gov/app/search/ . After downloading, I was able to download and the organize the data on RStudio.\nRStudio was also able to organize other variables of the data set in the Data tab.\nThe values set allows to see how long the dataset took long to load, in this case it took 9 seconds. We were also suggested to look at other storage methods like: Arrow, Feather, and Parquet. Arrow is a storage method that is able to hierarchically organize data on modern hardware. Feather is able “to read and write feather files,” at “maximum speed.” Lastly, Parquet similar to feather also manages columnar storage datasets, other than it also runs on Python."
  }
]